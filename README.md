# backend of science dashboard

Требования:
* `python 3.7.6 или новее`

чтобы произвести сборку проекта, нужно:

* `git clone`
* `pip install -r requirements.txt`
* `cd /api && git submodule add https://github.com/Zimovik007/scopus_fefu && git submodule init && git submodule update`
* `в корне проекта создать файл db.sqlite3`
* `python manage.py makemigrations`
* `python manage.py migrate`
* `создать папку data в корне проекта и добавить туда файлы: all_universities_russian_info.xlsx, subjects.xlsx, naukafefuids.xlsx, schools.xlsx, scopus_staff.xlsx, 1c_staff.xlsx Они подготавливались заранее.`
* `в папке data создать папку precalculation`
* `вызвать по-очереди методы из файла api/cron.py (время работы ~8-16 часов), такие как: download_all_fefu_documents, download_all_citations, download_all_authors, refill_db. Достигается это следующими командами:`
    * `python3 manage.py shell`
    * `from api.cron import download_all_fefu_documents`
    * `download_all_fefu_documents` (ждем; остальные скрипты аналогично) 
* `должны быть запущены cron задачи: (чтобы данные обновлялись раз в 2 недели; параметры запуска крона описаны в файле backend/settings.py)`
    * `проверка есть ли задачи: python3 manage.py crontab show`
    * `если задач нет - python3 manage.py crontab add`

на выходе должна получиться готовая БД `db.sqlite3` и собранный проект

# структура проекта

* `api/`
    * `models.py` - структура базы данных в синтаксисе django
    * `cron.py` - функции, которые должны выполняться кроном раз в 2 недели + некоторые функции, который я писал чтобы разово вызвать. Вроде бы из тех что нужно знать: `download_all_fefu_documents, download_all_citations, download_all_authors, refill_db`. Кажется, что названия дают о себе знать. Все данные, полученные этими функциями сохраняются в папку `data/`, либо обновляют `db.sqlite3`
    * `urls.py` - файл для роутов, в принципе в данной работе бесполезный, так как роут 1: `/auth_check`, он ведет в файл `auth.py`
    * `auth.py` - файл, который в перспективе должен проверять авторизацию пользователя и отдавать результат авторизации + данные дашборда.
    * `utilities.py` - содержит некоторые вспомогательные функции. На данные момент там только функции, помогающие заполнять базу данных. С самого начала разработки я не планировал делать для скриптов, заполняющих базу данных отдельную папку, поэтому при анализе структуры проекта может возникнуть вопрос, почему этот файл лежит отдельно от скриптов, заполняющих БД, ответ на этот вопрос: так исторически сложилось пока что.
    * остальные файлы в папке `/api` не представляют вроде бы интереса, это дефолтные сгенерированные джангой файлы для различных целей. По названиям понятно.
    * `api/db_utilities/`
        * в данной папке расположены скрипты, которые берут данные из папки `data/` и заполняют базу данных. Данные скрипты используются в файле `cron.py`. Создается экземпляр объекта `DBFiller`, к которому присоединяются экземпляры классов, каждый из которых отвечают за свою таблицу (см. файл `cron.py`) и вызывается метод, заполняющий всю БД. Побродно описывать каждый файл не вижу смысла, там каждый файл отвечает за своб таблицу и код воде был достаточно понятный. Берется файл из `data/`, заполняется по своим правилам и всё
        * Если понадобится еще одна таблица, то сначала ее нужно сконструировать в файле `api/models.py`, далее выполнить миграции: `python3 manage.py makemigrations`, `python3 manage.py migrate`; после этого нужно в `/api/db_utilities/`создать файл, который будет заполнять таблицу. Сделать нужно по аналогии с другими файлами, чтобы главные методы назывались также, а потом в `api/cron.py` испортировать этот класс и добавить по аналогии объект нового класса к объекту `DBFiller`.
    * `api/science/`
        * В данной директории происходит получение данных из базы данных, а также производятся не сложные рассчеты и отправляются на дашборд. Главным здесь является файл `science.py`, класс из которого вызывается из файла `api/auth.py` с методом `get_dict()`.
        * На данный момент реализована такая логика, что для каждой отдельной страницы дашборд в этой папке создается отдельный файл, который реализует фугкционал доставания из БД необходимых данных и формирования метрик. Например, `general_review.py` содержит в себе класс, который достает все данные, которые относятся к главной странице дашборда с главными метриками, а `author_review.py` - данные по странице авторов.
        * Кроме этого, было замечено, что метрики на главную страницу нельзя достать за короткое время, потому что требуют некоторые алгоритмически рассчеты над большим количеством данных, поэтому в файле `general_review.py` была реализована функция `make_precalculation`, которая делает рассчеты и сохраняет предрассчитанные данные в директорию `data/pre_calculation/10-s2.0-60103811`, где последняя поддиректория была сформирована из `eid` университета. Это сделано с заходом на возможное расширение на несколько университетов. Данная функция содержится внутри функции `api/cron.py refill_db`, вызов происходит следующим способом: `GeneralReview().make_precalculation()`. Если нужно будет сделать другие предрассчеты, их нужно выполнить соответствующим способом в своих классах и запуском таким же образом, что и `GeneralReview`.
* `backend/`
    * файлы в данном разделе являются дефолтными для любых джанго проектов, изменялся только файл `settings.py`, там содержится вся техническая конфигурация проекта. Документацию в интернете найти легко.
    * `backend/scopus_fefu` - это сабмодуль с функционалом скачивания со скопуса данных. Содержится в отдельном репозитории, к которому есть доступ у `Amadest` и `MrShepardd`
    